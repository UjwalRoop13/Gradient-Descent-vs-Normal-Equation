## Linear Regression: Normal Equation vs Gradient Descent

This module implements Linear Regression **from scratch** using NumPy.
It compares two optimization approaches:

- **Normal Equation** (closed-form solution)
- **Gradient Descent** (iterative optimization)

### Key Concepts
- Mean Squared Error cost function
- Matrix-based solution using normal equation
- Gradient descent convergence behavior
- Computational time comparison

### Insights
- Normal Equation converges instantly but is computationally expensive for large datasets.
- Gradient Descent scales better for large data but requires tuning of learning rate and iterations.

This implementation demonstrates a deep understanding of the mathematical foundations of machine learning.
